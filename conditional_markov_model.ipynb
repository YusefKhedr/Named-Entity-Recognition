{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Markov Model \n",
    "### A.k.a. Maximum Entropy Markov Models\n",
    "### Author: Omer Waseem\n",
    "#### Description: This Python notebook trains and evaluates a CMM using CoNLL and NEEL datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataprep import conll_sentences, conll_words, neel_sentences, neel_words\n",
    "from helper import accuracy, entity_count\n",
    "from nltk import MaxentClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CoNLL 2003 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_conll_features(index, sentence, pos, chunk):\n",
    "    \"\"\"Function used to extract features for the CoNLL dataset\n",
    "    \n",
    "    'w' represents word feature\n",
    "    't' represents POS tag feature\n",
    "    'c' represents chunk tag feature\n",
    "    '-n' represents previous 'n' feature\n",
    "    '+n' represents posterior 'n' feature\n",
    "    \"\"\"\n",
    "    \n",
    "    features = {}\n",
    "    last_index = len(sentence) - 1\n",
    "    word = sentence[index]\n",
    "    word_lc = word.lower()\n",
    "    \n",
    "    # features from current word:\n",
    "    features['w'] = word\n",
    "    features['t'] = pos[index]\n",
    "    features['length'] = len(word)\n",
    "    features['uppercase'] = any(x.isupper() for x in word)\n",
    "    features['firstletter'] = word[0].isupper() and (len(word) > 1)\n",
    "    features['hasdigits'] = any(x.isdigit() for x in word)\n",
    "    features['c'] = chunk[index]\n",
    "    features['loc_flag'] = ('field' in word_lc) or ('land' in word_lc) or ('burgh' in word_lc) or ('shire' in word_lc) \n",
    "    features['hasdot'] = ('.' in word and len(word) > 1)\n",
    "    features['endsinns'] = (len(word) > 1 and word_lc[-2:] == 'ns')\n",
    "    \n",
    "    \n",
    "    # features from previous 2 words\n",
    "    if index == 0: # first word in sentence\n",
    "        features['t-2 t-1'] = '<B> <B>'\n",
    "        features['t-1'] = '<B>'\n",
    "        features['w-2'] = '<B>'\n",
    "        features['w-1'] = '<B>'\n",
    "        features['c-2 c-1'] = '<B> <B>'\n",
    "        features['c-1'] = '<B>'\n",
    "    elif index == 1: # second word in sentence\n",
    "        features['t-2 t-1'] = '<B> ' + pos[0]\n",
    "        features['t-1'] = pos[0]\n",
    "        features['w-2'] = '<B>'\n",
    "        features['w-1'] = sentence[0]\n",
    "        features['c-2 c-1'] = '<B> ' + chunk[0]\n",
    "        features['c-1'] = chunk[0]\n",
    "    else:\n",
    "        features['t-2 t-1'] = pos[index-2] + ' ' + pos[index-1]\n",
    "        features['t-1'] = pos[index-1]\n",
    "        features['w-2'] = sentence[index-2]\n",
    "        features['w-1'] = sentence[index-1]\n",
    "        features['c-2 c-1'] = chunk[index-2] + ' ' + chunk[index-1]\n",
    "        features['c-1'] = chunk[index-1]\n",
    "\n",
    "      \n",
    "    # features from posterior 2 words\n",
    "    if index == last_index: # last word in sentence\n",
    "        features['t+1 t+2'] = '<E> <E>'\n",
    "        features['t+1'] = '<E>'\n",
    "        features['w+2'] = '<E>'\n",
    "        features['w+1'] = '<E>'\n",
    "    elif index == last_index - 1: # second to last word in sentence\n",
    "        features['t+1 t+2'] = pos[last_index] + ' <E>'\n",
    "        features['t+1'] = pos[last_index]\n",
    "        features['w+2'] = '<E>'\n",
    "        features['w+1'] = sentence[last_index]\n",
    "    else:\n",
    "        features['t+1 t+2'] = pos[index+1] + ' ' + pos[index+2]\n",
    "        features['t+1'] = pos[index+1]\n",
    "        features['w+2'] = sentence[index+2]\n",
    "        features['w+1'] = sentence[index+1]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get CoNLL training data from file and extract sentences with corresponding tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_train_file = './datasets/CoNLL2003/eng.train'\n",
    "c_train_sent, c_train_pos, c_train_chunk, c_train_entity = conll_sentences(c_train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each sentence create training data feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_train_data = []\n",
    "for sent, pos, chunk, entity in zip(c_train_sent, c_train_pos, c_train_chunk, c_train_entity): \n",
    "    if len(sent) != len(pos) or len(pos) != len(chunk) or len(chunk) != len(entity):\n",
    "        raise ValueError('error: CoNLL train length mismatch')  \n",
    "    for i, ent in enumerate(entity):\n",
    "        labelled_data = (get_conll_features(i, sent, pos, chunk), ent)\n",
    "        c_train_data.append(labelled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CMM using NLTK classifier and CoNLL training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object find_file_iter at 0x110e9c360>\n",
      "RuntimeError: generator ignored GeneratorExit\n"
     ]
    }
   ],
   "source": [
    "memm = MaxentClassifier.train(c_train_data, algorithm='MEGAM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate trained model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_testa_file = './datasets/CoNLL2003/eng.testa'\n",
    "c_testb_file = './datasets/CoNLL2003/eng.testb'\n",
    "c_testc_file = './datasets/CoNLL2003/eng.testc'\n",
    "\n",
    "c_testa_sent, c_testa_pos, c_testa_chunk, c_testa_entity = conll_sentences(c_testa_file)\n",
    "c_testb_sent, c_testb_pos, c_testb_chunk, c_testb_entity = conll_sentences(c_testb_file)\n",
    "c_testc_sent, c_testc_pos, c_testc_chunk, c_testc_entity = conll_sentences(c_testc_file)\n",
    "\n",
    "c_teata_truth = []\n",
    "c_testa_pred = []\n",
    "for sent, pos, chunk, entity in zip(c_testa_sent, c_testa_pos, c_testa_chunk, c_testa_entity):\n",
    "    if len(sent) != len(pos) or len(pos) != len(chunk) or len(chunk) != len(entity):\n",
    "        raise ValueError('error: CoNLL testa length mismatch')\n",
    "    for i, ent in enumerate(entity):\n",
    "        c_teata_truth.append(ent)\n",
    "        pred = memm.classify(get_conll_features(i, sent, pos, chunk))\n",
    "        c_testa_pred.append(pred)\n",
    "\n",
    "c_teatb_truth = []\n",
    "c_testb_pred = []\n",
    "for sent, pos, chunk, entity in zip(c_testb_sent, c_testb_pos, c_testb_chunk, c_testb_entity):\n",
    "    if len(sent) != len(pos) or len(pos) != len(chunk) or len(chunk) != len(entity):\n",
    "        raise ValueError('error: CoNLL testb length mismatch')\n",
    "    for i, ent in enumerate(entity):\n",
    "        c_teatb_truth.append(ent)\n",
    "        pred = memm.classify(get_conll_features(i, sent, pos, chunk))\n",
    "        c_testb_pred.append(pred)\n",
    "\n",
    "c_teatc_truth = []\n",
    "c_testc_pred = []\n",
    "for sent, pos, chunk, entity in zip(c_testc_sent, c_testc_pos, c_testc_chunk, c_testc_entity):\n",
    "    if len(sent) != len(pos) or len(pos) != len(chunk) or len(chunk) != len(entity):\n",
    "        raise ValueError('error: CoNLL testc length mismatch')\n",
    "    for i, ent in enumerate(entity):\n",
    "        c_teatc_truth.append(ent)\n",
    "        pred = memm.classify(get_conll_features(i, sent, pos, chunk))\n",
    "        c_testc_pred.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoNLL Evaluation\n",
    "#### testa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 48302 / 51362 = 0.940423\n"
     ]
    }
   ],
   "source": [
    "accuracy(c_teata_truth, c_testa_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.70308672,  0.76169265,  0.98155966,  0.64755392,  0.77272727]),\n",
       " array([ 0.68529131,  0.53943218,  0.98966299,  0.58843212,  0.83677358]),\n",
       " array([ 0.69407497,  0.63157895,  0.98559467,  0.61657901,  0.80347614]),\n",
       " array([ 2094,  1268, 42759,  2092,  3149]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(c_teata_truth, c_testa_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 43173 / 46435 = 0.929751\n"
     ]
    }
   ],
   "source": [
    "accuracy(c_teatb_truth, c_testb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.68520408,  0.66752911,  0.98136776,  0.65479332,  0.71844353]),\n",
       " array([ 0.69766234,  0.5620915 ,  0.97993372,  0.59655449,  0.81896863]),\n",
       " array([ 0.69137709,  0.61028977,  0.98065022,  0.62431866,  0.76541962]),\n",
       " array([ 1925,   918, 38323,  2496,  2773]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(c_teatb_truth, c_testb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 33 / 35 = 0.942857\n"
     ]
    }
   ],
   "source": [
    "accuracy(c_teatc_truth, c_testc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ow/anaconda/envs/datamgt/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/ow/anaconda/envs/datamgt/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.        ,  0.        ,  1.        ,  0.66666667,  1.        ]),\n",
       " array([ 0.,  0.,  1.,  1.,  1.]),\n",
       " array([ 0. ,  0. ,  1. ,  0.8,  1. ]),\n",
       " array([ 0,  2, 29,  2,  2]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(c_teatc_truth, c_testc_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NEEL 2006 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_neel_features(index, sentence):\n",
    "    \"\"\"Function used to extract features for the NEEL dataset\n",
    "    \n",
    "    'w' represents word feature\n",
    "    '-n' represents previous 'n' feature\n",
    "    '+n' represents posterior 'n' feature\n",
    "    \"\"\"\n",
    "    \n",
    "    features = {}\n",
    "    last_index = len(sentence) - 1\n",
    "    word = sentence[index]\n",
    "    word_lc = word.lower()\n",
    "    \n",
    "    # features from current word:\n",
    "    features['w'] = word\n",
    "    features['length'] = len(word)\n",
    "    features['uppercase'] = any(x.isupper() for x in word)\n",
    "    features['firstletter'] = word[0].isupper() and (len(word) > 1)\n",
    "    features['hasdigits'] = any(x.isdigit() for x in word)\n",
    "    features['loc_flag'] = ('field' in word_lc) or ('land' in word_lc) or ('burgh' in word_lc) or ('shire' in word_lc) \n",
    "    features['hasdot'] = ('.' in word and len(word) > 1)\n",
    "    features['endsinns'] = (len(word) > 1 and word_lc[-2:] == 'ns')\n",
    "    \n",
    "    \n",
    "    # features from previous 2 words\n",
    "    if index == 0: # first word in sentence\n",
    "        features['w-2'] = '<B>'\n",
    "        features['w-1'] = '<B>'\n",
    "    elif index == 1: # second word in sentence\n",
    "        features['w-2'] = '<B>'\n",
    "        features['w-1'] = sentence[0]\n",
    "    else:\n",
    "        features['w-2'] = sentence[index-2]\n",
    "        features['w-1'] = sentence[index-1]\n",
    "\n",
    "      \n",
    "    # features from posterior 2 words\n",
    "    if index == last_index: # last word in sentence\n",
    "        features['w+2'] = '<E>'\n",
    "        features['w+1'] = '<E>'\n",
    "    elif index == last_index - 1: # second to last word in sentence\n",
    "        features['w+2'] = '<E>'\n",
    "        features['w+1'] = sentence[last_index]\n",
    "    else:\n",
    "        features['w+2'] = sentence[index+2]\n",
    "        features['w+1'] = sentence[index+1]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get NEEL training data from files and extract sentences with corresponding tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_train_gs_file = './datasets/NEEL2006/training_neel.gs'\n",
    "n_train_tsv_file = './datasets/NEEL2006/training.tsv'\n",
    "n_train_sent, n_train_ent, n_train_err = neel_sentences(n_train_gs_file, n_train_tsv_file)\n",
    "\n",
    "n_train_data = []\n",
    "for sent, entity in zip(n_train_sent, n_train_ent): \n",
    "    if len(sent) != len(entity):\n",
    "        raise ValueError('error: NEEL train length mismatch')  \n",
    "    for i, ent in enumerate(entity):\n",
    "        labelled_data = (get_neel_features(i, sent), ent)\n",
    "        n_train_data.append(labelled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CMM on NEEL training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "memm = MaxentClassifier.train(n_train_data, algorithm='MEGAM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate trained model on NEEL test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_test_gs_file = './datasets/NEEL2006/test_neel.gs'\n",
    "n_test_tsv_file = './datasets/NEEL2006/test.tsv'\n",
    "n_test_sent, n_test_ent, n_test_err = neel_sentences(n_test_gs_file, n_test_tsv_file)\n",
    "\n",
    "n_test_truth = []\n",
    "n_test_pred = []\n",
    "for sent, entity in zip(n_test_sent, n_test_ent):\n",
    "    if len(sent) != len(entity):\n",
    "        raise ValueError('error: CoNLL testa length mismatch')\n",
    "    for i, ent in enumerate(entity):\n",
    "        n_test_truth.append(ent)\n",
    "        pred = memm.classify(get_conll_features(i, sent, pos, chunk))\n",
    "        n_test_pred.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 4665 / 5408 = 0.862611\n"
     ]
    }
   ],
   "source": [
    "accuracy(n_test_truth, n_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.66666667,  0.49094567,  0.913339  ,  0.36363636,  0.60555556]),\n",
       " array([ 0.21621622,  0.50413223,  0.98601238,  0.02739726,  0.28684211]),\n",
       " array([ 0.32653061,  0.49745158,  0.94828537,  0.05095541,  0.38928571]),\n",
       " array([  37,  484, 4361,  146,  380]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(n_test_truth, n_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG: 146\n",
      "PER: 380\n",
      "LOC: 37\n",
      "MISC: 484\n",
      "O: 4361\n"
     ]
    }
   ],
   "source": [
    "entity_count(n_test_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
